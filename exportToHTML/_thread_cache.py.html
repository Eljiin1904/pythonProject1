<html>
<head>
<title>_thread_cache.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_thread_cache.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">threading </span><span class="s0">import </span><span class="s1">Thread</span><span class="s0">, </span><span class="s1">Lock</span>
<span class="s0">import </span><span class="s1">outcome</span>
<span class="s0">from </span><span class="s1">itertools </span><span class="s0">import </span><span class="s1">count</span>

<span class="s2"># The &quot;thread cache&quot; is a simple unbounded thread pool, i.e., it automatically</span>
<span class="s2"># spawns as many threads as needed to handle all the requests its given. Its</span>
<span class="s2"># only purpose is to cache worker threads so that they don't have to be</span>
<span class="s2"># started from scratch every time we want to delegate some work to a thread.</span>
<span class="s2"># It's expected that some higher-level code will track how many threads are in</span>
<span class="s2"># use to avoid overwhelming the system (e.g. the limiter= argument to</span>
<span class="s2"># trio.to_thread.run_sync).</span>
<span class="s2">#</span>
<span class="s2"># To maximize sharing, there's only one thread cache per process, even if you</span>
<span class="s2"># have multiple calls to trio.run.</span>
<span class="s2">#</span>
<span class="s2"># Guarantees:</span>
<span class="s2">#</span>
<span class="s2"># It's safe to call start_thread_soon simultaneously from</span>
<span class="s2"># multiple threads.</span>
<span class="s2">#</span>
<span class="s2"># Idle threads are chosen in LIFO order, i.e. we *don't* spread work evenly</span>
<span class="s2"># over all threads. Instead we try to let some threads do most of the work</span>
<span class="s2"># while others sit idle as much as possible. Compared to FIFO, this has better</span>
<span class="s2"># memory cache behavior, and it makes it easier to detect when we have too</span>
<span class="s2"># many threads, so idle ones can exit.</span>
<span class="s2">#</span>
<span class="s2"># This code assumes that 'dict' has the following properties:</span>
<span class="s2">#</span>
<span class="s2"># - __setitem__, __delitem__, and popitem are all thread-safe and atomic with</span>
<span class="s2">#   respect to each other. This is guaranteed by the GIL.</span>
<span class="s2">#</span>
<span class="s2"># - popitem returns the most-recently-added item (i.e., __setitem__ + popitem</span>
<span class="s2">#   give you a LIFO queue). This relies on dicts being insertion-ordered, like</span>
<span class="s2">#   they are in py36+.</span>

<span class="s2"># How long a thread will idle waiting for new work before gives up and exits.</span>
<span class="s2"># This value is pretty arbitrary; I don't think it matters too much.</span>
<span class="s1">IDLE_TIMEOUT = </span><span class="s3">10  </span><span class="s2"># seconds</span>

<span class="s1">name_counter = count()</span>


<span class="s0">class </span><span class="s1">WorkerThread:</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">thread_cache):</span>
        <span class="s1">self._job = </span><span class="s0">None</span>
        <span class="s1">self._thread_cache = thread_cache</span>
        <span class="s2"># This Lock is used in an unconventional way.</span>
        <span class="s2">#</span>
        <span class="s2"># &quot;Unlocked&quot; means we have a pending job that's been assigned to us;</span>
        <span class="s2"># &quot;locked&quot; means that we don't.</span>
        <span class="s2">#</span>
        <span class="s2"># Initially we have no job, so it starts out in locked state.</span>
        <span class="s1">self._worker_lock = Lock()</span>
        <span class="s1">self._worker_lock.acquire()</span>
        <span class="s1">thread = Thread(target=self._work</span><span class="s0">, </span><span class="s1">daemon=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s1">thread.name = </span><span class="s4">f&quot;Trio worker thread </span><span class="s0">{</span><span class="s1">next(name_counter)</span><span class="s0">}</span><span class="s4">&quot;</span>
        <span class="s1">thread.start()</span>

    <span class="s0">def </span><span class="s1">_handle_job(self):</span>
        <span class="s2"># Handle job in a separate method to ensure user-created</span>
        <span class="s2"># objects are cleaned up in a consistent manner.</span>
        <span class="s1">fn</span><span class="s0">, </span><span class="s1">deliver = self._job</span>
        <span class="s1">self._job = </span><span class="s0">None</span>
        <span class="s1">result = outcome.capture(fn)</span>
        <span class="s2"># Tell the cache that we're available to be assigned a new</span>
        <span class="s2"># job. We do this *before* calling 'deliver', so that if</span>
        <span class="s2"># 'deliver' triggers a new job, it can be assigned to us</span>
        <span class="s2"># instead of spawning a new thread.</span>
        <span class="s1">self._thread_cache._idle_workers[self] = </span><span class="s0">None</span>
        <span class="s1">deliver(result)</span>

    <span class="s0">def </span><span class="s1">_work(self):</span>
        <span class="s0">while True</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">self._worker_lock.acquire(timeout=IDLE_TIMEOUT):</span>
                <span class="s2"># We got a job</span>
                <span class="s1">self._handle_job()</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s2"># Timeout acquiring lock, so we can probably exit. But,</span>
                <span class="s2"># there's a race condition: we might be assigned a job *just*</span>
                <span class="s2"># as we're about to exit. So we have to check.</span>
                <span class="s0">try</span><span class="s1">:</span>
                    <span class="s0">del </span><span class="s1">self._thread_cache._idle_workers[self]</span>
                <span class="s0">except </span><span class="s1">KeyError:</span>
                    <span class="s2"># Someone else removed us from the idle worker queue, so</span>
                    <span class="s2"># they must be in the process of assigning us a job - loop</span>
                    <span class="s2"># around and wait for it.</span>
                    <span class="s0">continue</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s2"># We successfully removed ourselves from the idle</span>
                    <span class="s2"># worker queue, so no more jobs are incoming; it's safe to</span>
                    <span class="s2"># exit.</span>
                    <span class="s0">return</span>


<span class="s0">class </span><span class="s1">ThreadCache:</span>
    <span class="s0">def </span><span class="s1">__init__(self):</span>
        <span class="s1">self._idle_workers = {}</span>

    <span class="s0">def </span><span class="s1">start_thread_soon(self</span><span class="s0">, </span><span class="s1">fn</span><span class="s0">, </span><span class="s1">deliver):</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s1">worker</span><span class="s0">, </span><span class="s1">_ = self._idle_workers.popitem()</span>
        <span class="s0">except </span><span class="s1">KeyError:</span>
            <span class="s1">worker = WorkerThread(self)</span>
        <span class="s1">worker._job = (fn</span><span class="s0">, </span><span class="s1">deliver)</span>
        <span class="s1">worker._worker_lock.release()</span>


<span class="s1">THREAD_CACHE = ThreadCache()</span>


<span class="s0">def </span><span class="s1">start_thread_soon(fn</span><span class="s0">, </span><span class="s1">deliver):</span>
    <span class="s5">&quot;&quot;&quot;Runs ``deliver(outcome.capture(fn))`` in a worker thread. 
 
    Generally ``fn`` does some blocking work, and ``deliver`` delivers the 
    result back to whoever is interested. 
 
    This is a low-level, no-frills interface, very similar to using 
    `threading.Thread` to spawn a thread directly. The main difference is 
    that this function tries to re-use threads when possible, so it can be 
    a bit faster than `threading.Thread`. 
 
    Worker threads have the `~threading.Thread.daemon` flag set, which means 
    that if your main thread exits, worker threads will automatically be 
    killed. If you want to make sure that your ``fn`` runs to completion, then 
    you should make sure that the main thread remains alive until ``deliver`` 
    is called. 
 
    It is safe to call this function simultaneously from multiple threads. 
 
    Args: 
 
        fn (sync function): Performs arbitrary blocking work. 
 
        deliver (sync function): Takes the `outcome.Outcome` of ``fn``, and 
          delivers it. *Must not block.* 
 
    Because worker threads are cached and reused for multiple calls, neither 
    function should mutate thread-level state, like `threading.local` objects 
    â€“ or if they do, they should be careful to revert their changes before 
    returning. 
 
    Note: 
 
        The split between ``fn`` and ``deliver`` serves two purposes. First, 
        it's convenient, since most callers need something like this anyway. 
 
        Second, it avoids a small race condition that could cause too many 
        threads to be spawned. Consider a program that wants to run several 
        jobs sequentially on a thread, so the main thread submits a job, waits 
        for it to finish, submits another job, etc. In theory, this program 
        should only need one worker thread. But what could happen is: 
 
        1. Worker thread: First job finishes, and calls ``deliver``. 
 
        2. Main thread: receives notification that the job finished, and calls 
           ``start_thread_soon``. 
 
        3. Main thread: sees that no worker threads are marked idle, so spawns 
           a second worker thread. 
 
        4. Original worker thread: marks itself as idle. 
 
        To avoid this, threads mark themselves as idle *before* calling 
        ``deliver``. 
 
        Is this potential extra thread a major problem? Maybe not, but it's 
        easy enough to avoid, and we figure that if the user is trying to 
        limit how many threads they're using then it's polite to respect that. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">THREAD_CACHE.start_thread_soon(fn</span><span class="s0">, </span><span class="s1">deliver)</span>
</pre>
</body>
</html>